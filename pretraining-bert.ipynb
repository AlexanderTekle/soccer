{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33e3ad5-b6a3-4117-a9ed-817144415f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file './raw_data/combined_unlabeled_commentary_clean.txt' has 3962 words.\n"
     ]
    }
   ],
   "source": [
    "def get_word_count(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    # Read the entire file content\n",
    "    content = f.read()\n",
    "\n",
    "    # Split the content into words, removing whitespaces\n",
    "    words = content.split()\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "# Example usage\n",
    "filename = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Replace with your actual filename\n",
    "word_count = get_word_count(filename)\n",
    "print(f\"The file '{filename}' has {word_count} words.\")\n",
    "# print(f\"The file '{filename2}' has {word_count2} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d246e04d-0188-45bd-ab97-ec5a9e24d5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bf6f295-5cc6-413d-84c3-a8c4710b18af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file split into sentences and saved to ./raw_data/combined_unlabeled_commentary_clean.txt\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(input_file, output_file):\n",
    "  \"\"\"\n",
    "  Splits a text file into separate lines for each sentence.\n",
    "\n",
    "  Args:\n",
    "    input_file (str): Path to the input text file.\n",
    "    output_file (str): Path to the output file where the split sentences will be written.\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  current_sentence = \"\"\n",
    "  with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "      # Add line to current sentence (excluding trailing newline)\n",
    "      current_sentence += line.strip()\n",
    "\n",
    "      # Check for sentence delimiters (e.g., ., ?, !) and write complete sentences\n",
    "      for delimiter in [\".\", \"?\", \"!\"]:\n",
    "        if delimiter in current_sentence:\n",
    "          if (len(sentences) < 100 and len(current_sentence) > 8):\n",
    "            sentences.append(current_sentence.split(delimiter)[0] + delimiter + \"\\n\")\n",
    "          current_sentence = current_sentence.split(delimiter)[1].strip()\n",
    "\n",
    "      # Write any remaining sentence at the end of the file\n",
    "      # if current_sentence:\n",
    "      #   sentences.append(current_sentence)\n",
    "    outfile.writelines(sentences)\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"./raw_data/combined_unlabeled_commentary.txt\"\n",
    "output_file = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Output file with split sentences\n",
    "\n",
    "split_sentences(input_file, output_file)\n",
    "\n",
    "print(f\"Text file split into sentences and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c238d47-68de-455a-a355-e54d19c1d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = get_word_count(output_file)\n",
    "word_count\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0235566-95fc-47ae-bbf9-be5316b6b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a custom dataset class\n",
    "class TextDataset(Dataset):\n",
    "  def __init__(self, filepath):\n",
    "    self.sentences = []\n",
    "    with open(filepath, 'r') as f:\n",
    "      for line in f:\n",
    "        self.sentences.append(line.strip())\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sentence = self.sentences[idx]\n",
    "    #print(sentence)\n",
    "\n",
    "    # Tokenize the sentence\n",
    "    tokenized_sentence = tokenizer.tokenize(sentence, padding='max_length', return_tensors='pt')\n",
    "    # Apply MLM (replace some tokens with [MASK])\n",
    "    masked_sentence = mask_tokens(tokenized_sentence, tokenizer)\n",
    "    # Convert tokens to ids\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(masked_sentence)\n",
    "    # Convert labels (masked tokens) to ids\n",
    "    labels = tokenizer.convert_tokens_to_ids([t for t in masked_sentence if t == '[MASK]'])\n",
    "    attention_mask = [1] * len(input_ids)  # All tokens are attended to\n",
    "    print(input_ids)\n",
    "\n",
    "    # Move tensors to cuda device\n",
    "    # input_ids = input_ids.to(device)\n",
    "    # labels = labels.to(device)\n",
    "    # attention_mask = attention_mask.to(device)\n",
    "    return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(),\n",
    "            'label': labels.flatten()}\n",
    "\n",
    "# Define function to mask tokens\n",
    "def mask_tokens(tokens, tokenizer):\n",
    "  \"\"\"\n",
    "  Performs MLM by masking a certain percentage of tokens.\n",
    "  \"\"\"\n",
    "  # Replace some tokens with [MASK]\n",
    "  cand_mask_idx = [i for i, t in enumerate(tokens) if t not in [tokenizer.cls_token, tokenizer.sep_token]]\n",
    "  # Adjust masking rate based on your needs (e.g., 15% for BERT pre-training)\n",
    "  num_to_mask = int(0.15 * len(cand_mask_idx))\n",
    "  rng = torch.rand(len(cand_mask_idx))\n",
    "  masked_tokens = [t if rng[i] > 0.8 else '[MASK]' for i, t in enumerate(tokens)]\n",
    "  # Replace 10% of masked tokens with random words (optional)\n",
    "  for i in range(num_to_mask // 10):\n",
    "    masked_tokens[torch.randint(len(cand_mask_idx), size=(1,))[0]] = random.choice(list(tokenizer.vocab.keys()))\n",
    "  return masked_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "726fcf1a-4595-4821-8cb3-b3bdf91e1044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load your text file\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mTextDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create dataloader\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[45], line 15\u001b[0m, in \u001b[0;36mTextDataset.__init__\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m)):\n\u001b[1;32m     16\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentences\u001b[38;5;241m.\u001b[39mappend(line\u001b[38;5;241m.\u001b[39mstrip())\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'  # Replace with your desired model\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "# Move model to cuda device\n",
    "model.to(device)\n",
    "\n",
    "# Define training parameters\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "print(\"here3\")\n",
    "# Load your text file\n",
    "dataset = TextDataset(filename)\n",
    "print(\"here2\")\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"here1\")\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855e7042-f9f7-4ac3-aaaa-de85c3ebe81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'padding': 'max_length', 'return_tensors': 'pt'} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "test2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'flatten'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      5\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Forward pass (move tensors to device before feeding to model)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3-8-19/lib/python3.8/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3-8-19/lib/python3.8/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3-8-19/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/python3-8-19/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[14], line 39\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Move tensors to cuda device\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# input_ids = input_ids.to(device)\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# labels = labels.to(device)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# attention_mask = attention_mask.to(device)\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: attention_mask\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: labels\u001b[38;5;241m.\u001b[39mflatten()}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'flatten'"
     ]
    }
   ],
   "source": [
    "print(\"here\")\n",
    "# Training loop\n",
    "i = 0\n",
    "for epoch in range(num_epochs):\n",
    "  print(\"test2\")\n",
    "  for step, batch in enumerate(dataloader):\n",
    "    print(\"test\")\n",
    "    # Forward pass (move tensors to device before feeding to model)\n",
    "    if i%1000 == 0:\n",
    "      print(tokenizer.decode(input_ids[0]))\n",
    "      # print(input_ids)\n",
    "      # print(labels)\n",
    "      # print(attention_mask)\n",
    "      \n",
    "    i+=1\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['attention_mask'].to(device)\n",
    "    attention_mask = batch['label'].to(device)\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = criterion(outputs.logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress (optional)\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70efd564-9fc5-457e-ac29-0b0a1595335a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1342594628.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[61], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c949d248-ddc8-4605-8d22-7be682a4bcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef158cf-ca39-4176-8d06-7ec5e3b5cb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885eb515-3ca1-4d17-b16c-8f30354b9c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d1023f9-e4ff-4a8a-8aec-6583f7a064a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = \"mps\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Define tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c11328b-97f5-4041-8745-2fdbef7cac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class MLMDataset(Dataset):\n",
    "  def __init__(self, tokenizer, filepath):\n",
    "    self.tokenizer = tokenizer\n",
    "    self.sentences = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "      for line in f:\n",
    "        # Clean and strip whitespaces from each line\n",
    "        clean_line = line.strip()\n",
    "        if clean_line and len(clean_line) > 8:  # Add only non-empty lines\n",
    "          self.sentences.append(clean_line)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    sentence = self.sentences[idx]\n",
    "    # print(idx)\n",
    "    # print(sentence + \"\\n\")\n",
    "    encoding = self.tokenizer(sentence, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    print(encoding[\"attention_mask\"])\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# Create dataset object\"\n",
    "dataset = MLMDataset(tokenizer, \"./raw_data/combined_unlabeled_commentary_clean.txt\")\n",
    "\n",
    "# Create DataCollator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "\n",
    "# Define training loop parameters\n",
    "batch_size = 8\n",
    "learning_rate = 2e-5\n",
    "epochs = 3\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define optimizer\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d7f698-8051-4830-9f7b-13495cf6aeea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  print(f\"Epoch: {epoch+1}\")\n",
    "  for batch in train_dataloader:\n",
    "    print(\"working\")\n",
    "    # Move batch data to device\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    print(input_ids.shape)\n",
    "    input_ids.squeeze(0)\n",
    "    print(input_ids.shape)\n",
    "\n",
    "    # Forward pass\n",
    "    loss, logits  = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    #loss = outputs.loss\n",
    "\n",
    "    # Backward pass and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print loss\n",
    "    print(f\"Batch loss: {loss.item():.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62167ae-b028-48a4-8632-2b94afbaf7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf6baf-bb58-4b1f-9208-75f84db2ae11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6bc4ed90-20ca-41e8-9e4c-eb12bebd9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Define paths and hyperparameters\n",
    "model_name = \"bert-base-uncased\"  # Pre-trained BERT model\n",
    "text_file = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Path to your text file\n",
    "batch_size = 8  # Adjust this based on your GPU memory\n",
    "max_len = 512  # Maximum sequence length\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Load tokenizer and model on chosen device\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9b04829e-6042-4397-abf8-59f17f0c7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to mask tokens\n",
    "def mask_sentence(sentence):\n",
    "  \"\"\"Masks 15% of the tokens in a sentence.\"\"\"\n",
    "  # print(\"ok\")\n",
    "  # print(sentence[\"text\"])\n",
    "  # print(type(sentence[\"text\"]))\n",
    "  masked_sentence = tokenizer(sentence[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "  masked_sentence = masked_sentence.to(device)  # Move data to device\n",
    "\n",
    "  input_ids = masked_sentence[\"input_ids\"]\n",
    "  attention_mask = masked_sentence[\"attention_mask\"]\n",
    "  labels = input_ids.clone()\n",
    "\n",
    "  # Randomly mask 15% of tokens\n",
    "  probability_mask = 0.15\n",
    "  mask_indices = torch.bernoulli(torch.ones_like(input_ids) * probability_mask).bool()\n",
    "  labels[~mask_indices] = -100  # Set padding tokens to -100\n",
    "\n",
    "  # Replace masked tokens with [MASK]\n",
    "  input_ids[mask_indices] = tokenizer.mask_token_id\n",
    "\n",
    "  return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "4df1b08a-f2e9-464f-8afc-d97c485e339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data from text file\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": text_file}, split=\"train\")\n",
    "train_data = dataset.map(mask_sentence, batched=True)\n",
    "print(type(train_data))\n",
    "train_data.set_format(\"torch\")\n",
    "\n",
    "# Remove 'text' column and set format to 'torch'\n",
    "print(type(train_data))\n",
    "print(train_data)\n",
    "train_data = train_data.remove_columns([\"text\"])\n",
    "print(type(train_data))\n",
    "print(train_data)\n",
    "print(type(train_data[0]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "\n",
    "# print(train_data)\n",
    "# #print(train_data['text'][0])\n",
    "# print(train_data['input_ids'][0])\n",
    "# print(train_data['attention_mask'][0])\n",
    "# print(train_data['labels'][0])\n",
    "# print(len(train_data))\n",
    "\n",
    "# train_data = train_data.remove_columns([\"text\"])\n",
    "# #train_data = train_data.set_format(type='torch', columns=['input_ids','attention_mask','labels'])\n",
    "\n",
    "# # # Define optimizer and training loop (replace with your training logic)\n",
    "# optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# print(type(train_data))\n",
    "# print(train_data)\n",
    "# #print(train_data['text'][0])\n",
    "# print(train_data['input_ids'][0])\n",
    "# print(train_data['attention_mask'][0])\n",
    "# print(train_data['labels'][0])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "2a8792a9-2a0a-4b0a-b159-24bf078dd455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 13\n"
     ]
    }
   ],
   "source": [
    "num_batches = len(train_dataloader)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03a905f-67d6-4181-8581-6ff4e4b90164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64b05af-c247-46a7-b43f-c621c78359c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a18fd44f-a866-4539-bc26-7e5bf498bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 8.32705020904541\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 10.645683288574219\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 11.859464645385742\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 10.321561813354492\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 9.99863338470459\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 11.230396270751953\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 13.147407531738281\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 11.418228149414062\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 12.357796669006348\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 8.758637428283691\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 9.62027359008789\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 9.782269477844238\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 10.08958625793457\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 10.690547943115234\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 6.596676826477051\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 12.381726264953613\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 11.211284637451172\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 8.51325511932373\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 9.502029418945312\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 1, Loss: 8.909448623657227\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.943596839904785\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 10.301560401916504\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.127921104431152\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.936933517456055\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 12.828001976013184\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 12.793844223022461\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.29849910736084\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 10.168855667114258\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 7.669916152954102\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 10.376895904541016\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 11.18338680267334\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.455945014953613\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 8.326830863952637\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 9.922554016113281\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 11.604304313659668\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 11.768877983093262\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 10.645651817321777\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 11.481163024902344\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 8.594182968139648\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 2, Loss: 10.562368392944336\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 9.186416625976562\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.457865715026855\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 11.131274223327637\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.862224578857422\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 8.831985473632812\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 9.97811508178711\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.238892555236816\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.073297500610352\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 12.1747407913208\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 9.809194564819336\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 11.295646667480469\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 11.339975357055664\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.335320472717285\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 8.653792381286621\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.05158805847168\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.020424842834473\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.7584867477417\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 11.459701538085938\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 8.875092506408691\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "<class 'torch.Tensor'>\n",
      "5\n",
      "5\n",
      "5\n",
      "Epoch: 3, Loss: 10.305073738098145\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):  # Adjust number of epochs\n",
    "  for batch in train_loader:\n",
    "    # Move data to device\n",
    "    #batch.to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    print(type(input_ids))\n",
    "    print(type(attention_mask))\n",
    "    print(type(labels))\n",
    "    print(len(input_ids))\n",
    "    print (len(attention_mask))\n",
    "    print (len(labels))\n",
    "    #batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids= input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress (optional)\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "model.save_pretrained(\"masked_lm_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce4eca48-852d-47ca-9d7d-36f49017993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf67efd77c44546886f13a00df42038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['a', 'b'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_dict({\"a\": [\"ok\", \"okk\", \"2\"]})\n",
    "print(type(dataset))\n",
    "print(type(dataset[\"a\"]))\n",
    "test = dataset.map(lambda batch: {\"b\": batch[\"a\"]})\n",
    "print(test)\n",
    "#train_data = dataset.map(mask_sentence)\n",
    "\n",
    "\n",
    "# second = dataset.map(lambda batch: {\"b\": batch[\"a\"]}, batched=True)\n",
    "# print(type(second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ac05b-b6e3-4b0d-8436-1558c20412a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3-8-19",
   "language": "python",
   "name": "python-3-8-19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
