{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b33e3ad5-b6a3-4117-a9ed-817144415f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file './raw_data/combined_unlabeled_commentary_clean.txt' has 3962 words.\n"
     ]
    }
   ],
   "source": [
    "def get_word_count(filename):\n",
    "  with open(filename, 'r') as f:\n",
    "    # Read the entire file content\n",
    "    content = f.read()\n",
    "\n",
    "    # Split the content into words, removing whitespaces\n",
    "    words = content.split()\n",
    "\n",
    "    return len(words)\n",
    "\n",
    "# Example usage\n",
    "filename = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Replace with your actual filename\n",
    "word_count = get_word_count(filename)\n",
    "print(f\"The file '{filename}' has {word_count} words.\")\n",
    "# print(f\"The file '{filename2}' has {word_count2} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6bf6f295-5cc6-413d-84c3-a8c4710b18af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file split into sentences and saved to ./raw_data/combined_unlabeled_commentary_clean.txt\n"
     ]
    }
   ],
   "source": [
    "def split_sentences(input_file, output_file):\n",
    "  \"\"\"\n",
    "  Splits a text file into separate lines for each sentence.\n",
    "\n",
    "  Args:\n",
    "    input_file (str): Path to the input text file.\n",
    "    output_file (str): Path to the output file where the split sentences will be written.\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  current_sentence = \"\"\n",
    "  with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "    for line in infile:\n",
    "      # Add line to current sentence (excluding trailing newline)\n",
    "      current_sentence += line.strip()\n",
    "\n",
    "      # Check for sentence delimiters (e.g., ., ?, !) and write complete sentences\n",
    "      for delimiter in [\".\", \"?\", \"!\"]:\n",
    "        if delimiter in current_sentence:\n",
    "          if (len(sentences) < 100 and len(current_sentence) > 8):\n",
    "            sentences.append(current_sentence.split(delimiter)[0] + delimiter + \"\\n\")\n",
    "          current_sentence = current_sentence.split(delimiter)[1].strip()\n",
    "\n",
    "      # Write any remaining sentence at the end of the file\n",
    "      # if current_sentence:\n",
    "      #   sentences.append(current_sentence)\n",
    "    outfile.writelines(sentences)\n",
    "\n",
    "# Example usage:\n",
    "input_file = \"./raw_data/combined_unlabeled_commentary.txt\"\n",
    "output_file = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Output file with split sentences\n",
    "\n",
    "split_sentences(input_file, output_file)\n",
    "\n",
    "print(f\"Text file split into sentences and saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c238d47-68de-455a-a355-e54d19c1d32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4061"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = get_word_count(output_file)\n",
    "word_count\n",
    "\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6bc4ed90-20ca-41e8-9e4c-eb12bebd9094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# Define paths and hyperparameters\n",
    "model_name = \"bert-base-uncased\"  # Pre-trained BERT model\n",
    "text_file = \"./raw_data/combined_unlabeled_commentary_clean.txt\"  # Path to your text file\n",
    "batch_size = 8  # Adjust this based on your GPU memory\n",
    "max_len = 512  # Maximum sequence length\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Load tokenizer and model on chosen device\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForMaskedLM.from_pretrained(model_name).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "9b04829e-6042-4397-abf8-59f17f0c7311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to mask tokens\n",
    "def mask_sentence(sentence):\n",
    "  \"\"\"Masks 15% of the tokens in a sentence.\"\"\"\n",
    "  masked_sentence = tokenizer(sentence[\"text\"], return_tensors=\"pt\", truncation=True, padding=True)\n",
    "  masked_sentence = masked_sentence.to(device)  # Move data to device\n",
    "\n",
    "  input_ids = masked_sentence[\"input_ids\"]\n",
    "  attention_mask = masked_sentence[\"attention_mask\"]\n",
    "  labels = input_ids.clone()\n",
    "\n",
    "  # Randomly mask 15% of tokens\n",
    "  probability_mask = 0.15\n",
    "  mask_indices = torch.bernoulli(torch.ones_like(input_ids) * probability_mask).bool()\n",
    "  labels[~mask_indices] = -100  # Set padding tokens to -100\n",
    "\n",
    "  # Replace masked tokens with [MASK]\n",
    "  input_ids[mask_indices] = tokenizer.mask_token_id\n",
    "\n",
    "  return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "4df1b08a-f2e9-464f-8afc-d97c485e339e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 100\n",
      "})\n",
      "<class 'dict'>\n",
      "Number of batches: 13\n"
     ]
    }
   ],
   "source": [
    "# Load data from text file\n",
    "dataset = load_dataset(\"text\", data_files={\"train\": text_file}, split=\"train\")\n",
    "train_data = dataset.map(mask_sentence, batched=True)\n",
    "print(type(train_data))\n",
    "train_data.set_format(\"torch\")\n",
    "\n",
    "print(type(train_data))\n",
    "print(train_data)\n",
    "print(type(train_data))\n",
    "print(train_data)\n",
    "print(type(train_data[0]))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)\n",
    "num_batches = len(train_dataloader)\n",
    "print(f\"Number of batches: {num_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18fd44f-a866-4539-bc26-7e5bf498bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 8.6705322265625\n",
      "Epoch: 1, Loss: 9.833917617797852\n",
      "Epoch: 1, Loss: 11.892194747924805\n",
      "Epoch: 1, Loss: 10.5023193359375\n",
      "Epoch: 1, Loss: 9.986804008483887\n",
      "Epoch: 1, Loss: 7.155943393707275\n",
      "Epoch: 1, Loss: 8.5454683303833\n",
      "Epoch: 1, Loss: 8.85640811920166\n",
      "Epoch: 1, Loss: 8.859716415405273\n",
      "Epoch: 1, Loss: 7.355210781097412\n",
      "Epoch: 1, Loss: 10.656970977783203\n",
      "Epoch: 1, Loss: 9.582688331604004\n",
      "Epoch: 1, Loss: 8.176599502563477\n",
      "Epoch: 1, Loss: 7.87153434753418\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # Adjust number of epochs\n",
    "  for batch in train_loader:\n",
    "    # Move data to device\n",
    "    #batch.to(device)\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    #print(batch[\"text\"])\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids= input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = outputs.loss\n",
    "\n",
    "    # Backward pass and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training progress (optional)\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss}\")\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "model.save_pretrained(\"masked_lm_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ce4eca48-852d-47ca-9d7d-36f49017993c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf67efd77c44546886f13a00df42038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['a', 'b'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ac05b-b6e3-4b0d-8436-1558c20412a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3-8-19",
   "language": "python",
   "name": "python-3-8-19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
