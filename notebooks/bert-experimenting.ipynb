{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4469aaee-0415-48bf-9f07-5aa93fb71162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: USE_MPS=1\n",
      "env: USE_PYTORCH_METAL=1\n"
     ]
    }
   ],
   "source": [
    "%env USE_MPS=1\n",
    "\n",
    "%env USE_PYTORCH_METAL=1\n",
    "\n",
    "# %env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.7 ./webui.sh --precision full --no-half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e13ec1e-27a4-4fbd-a7de-7c44d3b14259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.3\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2f0a449-7ce1-4a2d-bb23-a2a506df79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4a5a527-9e02-4a57-a0e2-2d1c14505b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "#verify mps is available\n",
    "import torch\n",
    "torch.set_default_device(\"mps\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e8df03c-35b3-42a2-99f0-8bcdf665b7d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 6838\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 3259\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ID', 'Tweet', 'anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', 'optimism', 'pessimism', 'sadness', 'surprise', 'trust'],\n",
       "        num_rows: 886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### import torch\n",
    "from transformers import BertModel \n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "# model = BertModel.from_pretrained(\"bert-base-uncased\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n",
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\").with_format(\"torch\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9300a507-03f4-4eba-8745-2cf045841338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c88e8c4-6902-4328-8622-bcb2d5c6fa9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2017-En-21441',\n",
       " 'Tweet': \"â€œWorry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'anger': tensor(False, device='mps:0'),\n",
       " 'anticipation': tensor(True, device='mps:0'),\n",
       " 'disgust': tensor(False, device='mps:0'),\n",
       " 'fear': tensor(False, device='mps:0'),\n",
       " 'joy': tensor(False, device='mps:0'),\n",
       " 'love': tensor(False, device='mps:0'),\n",
       " 'optimism': tensor(True, device='mps:0'),\n",
       " 'pessimism': tensor(False, device='mps:0'),\n",
       " 'sadness': tensor(False, device='mps:0'),\n",
       " 'surprise': tensor(False, device='mps:0'),\n",
       " 'trust': tensor(True, device='mps:0')}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset\n",
    "# dataset['train'][0]\n",
    "# dataset['train'].set_format(\"torch\", device=\"mps\") \n",
    "# dataset['validation'].set_format(\"torch\", device=\"mps\") \n",
    "# dataset.set_format(\"torch\", device=\"mps\") \n",
    "\n",
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b34ea7b-2238-4dc9-95e5-14fb31b93865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'anticipation',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'joy',\n",
       " 'love',\n",
       " 'optimism',\n",
       " 'pessimism',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'trust']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['ID', 'Tweet']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf586d6-b058-41d2-93d1-94b17c60c36d",
   "metadata": {},
   "source": [
    "Now we are getting ready to preprocess the data using the BERT tokenizer. This include mapping the text to float ing point labels and moving it into a matrix of size batch_size x num_labels. These should be floats per PyTorch expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3faca06f-1226-4c3e-86b3-90cef8c2f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    #get the text of this batch\n",
    "    text=examples[\"Tweet\"]\n",
    "    #get the ecoding for this text tusing bert tokenizer\n",
    "    encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length = 128)\n",
    "    #add labels\n",
    "    labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
    "    #numpy array size batch x num labels\n",
    "    labels_matrix = np.zeros((len(text), len(labels)))\n",
    "\n",
    "    # replace zeros in numpy array with values from encoding\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "\n",
    "    encoding[\"labels\"] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc3c030-2c15-40f2-a031-f655bdfb4ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = dataset['train'][5]['Tweet']\n",
    "len(text)\n",
    "# encoded_dataset['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12db118d-a036-4400-b1cf-8f06ec703eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 6838\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3259\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 886\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)\n",
    "# encoded_dataset\n",
    "# encoded_dataset[\"train\"]\n",
    "# encoded_dataset[\"validation\"]\n",
    "# encoded_dataset[\"train\"][\"input_ids\"]\n",
    "# encoded_dataset.with_format(\"torch\")\n",
    "# torch.Tensor(encoded_dataset)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f12a55-c0c5-47d9-84cd-64799eb05883",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'].set_format(\"torch\", device=\"mps\") \n",
    "encoded_dataset['validation'].set_format(\"torch\", device=\"mps\") \n",
    "encoded_dataset.set_format(\"torch\", device=\"mps\") \n",
    "encoded_dataset[\"train\"]\n",
    "encoded_dataset[\"validation\"]\n",
    "encoded_dataset[\"train\"][\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c12d174-0585-475e-9d37-410c982e4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = encoded_dataset['train'][0]\n",
    "print(example.keys())\n",
    "\n",
    "len(example['input_ids'])\n",
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3979663-73f4-4ef8-855e-b01600980276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(example['input_ids'])\n",
    "#CLS = classify token and it is placed at the beginning of input\n",
    "#SEP = end of string used for next sentence prediction\n",
    "#PAD = pad to 128 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875851ae-4303-47dc-a210-da260efcc504",
   "metadata": {},
   "outputs": [],
   "source": [
    "example['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed831f78-7756-433e-b8fd-09375cb2aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc1a5a1-49a1-4dcd-81f0-f1a51ce855c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b8cdbdb-9a25-4397-9853-708c056b06c5",
   "metadata": {},
   "source": [
    "Setup model. multi_label_classification indicates the type of problem. We'll use BCEWithLogitsLoss (sigmoid layer with binary cross entropy loss - BCEWithLogitsLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb5052-17ea-4b52-a4dd-c4aafe782372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", \n",
    "                                                           problem_type=\"multi_label_classification\", \n",
    "                                                           num_labels=len(labels),\n",
    "                                                           id2label=id2label,\n",
    "                                                           label2id=label2id)\n",
    "model = model.to(device)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c3a403-4483-4402-8d56-5b492f952e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f56d5-53d8-4c40-98bd-99af3f93d932",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    use_mps_device=True\n",
    "    #push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f9e47-3494-4466-b269-9ae0c18ff3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid(device=model.device)\n",
    "    probs = sigmoid(torch.Tensor(predictions, device=device))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce549f8-9a78-49bd-aa89-3ebc58d94022",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset['train'][0]['labels'].type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6637f2c-26d1-48cc-a093-e3aa190928fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5333e0-e77a-4a5a-887e-06f5aec427fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "train_dataset = TensorDataset(encoded_dataset[\"train\"][\"input_ids\"], \n",
    "                             encoded_dataset[\"train\"][\"attention_mask\"],\n",
    "                             encoded_dataset[\"train\"][\"labels\"]);\n",
    "batch_size = 16  # Adjust based on your GPU memory\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "eval_loader = DataLoader(encoded_dataset[\"validation\"], batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# train_loader.to(device)\n",
    "\n",
    "# encoded_dataset['train'].set_format(\"torch\", device=\"mps\")\n",
    "# encoded_dataset['validation'].set_format(\"torch\", device=\"mps\")\n",
    "#dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d865f2-0d00-4c86-892c-290c0f7c42cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), \n",
    "#                 labels=encoded_dataset['train'][0]['labels'].unsqueeze(0),\n",
    "#                 attention_mask=encoded_dataset['train'][0]['attention_mask'].unsqueeze(0))\n",
    "\n",
    "encoded_dataset['train'].set_format(\"torch\", device=\"mps\")\n",
    "encoded_dataset['train'][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fcf239-71d5-4669-980e-d3585c621142",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b79afb3-3e02-4b22-a401-9471c29611e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72966577-2650-472d-a012-73ce856b6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = set(torch.tensor(np.unique(encoded_dataset['train'][\"labels\"].cpu())).cpu())\n",
    "\n",
    "# one = set(np.unique(encoded_dataset['train'][\"labels\"].cpu()))\n",
    "# one \n",
    "one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154733c2-3c93-45d9-8961-f87d52de6b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()\n",
    "two = set(encoded_dataset['train'][\"labels\"].cpu())\n",
    "two\n",
    "# two = set(encoded_dataset['train'][\"labels\"].cpu())\n",
    "# two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c15344-3dfe-44eb-bd93-4309a20a25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two - one\n",
    "\n",
    "# if (two - one):\n",
    "#     print(\"hello\")\n",
    "# else: \n",
    "#     print(\"helloooo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35791d1c-f616-4b24-931e-c99ad96a41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# #compute the class weights\n",
    "# encoded_dataset['train'][\"labels\"]\n",
    "# torch.tensor(encoded_dataset['train'][\"labels\"]).tolist()\n",
    "# class_wts = compute_class_weight(class_weight = 'balanced', \n",
    "#                                  classes = np.unique(encoded_dataset['train'][\"labels\"].cpu()),\n",
    "#                                  y = encoded_dataset['train'][\"labels\"].cpu())\n",
    "\n",
    "# # print(class_wts)\n",
    "# # weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "# # weights = weights.to(device)\n",
    "\n",
    "# # print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e2e54-4093-4b23-a9fe-28cea094d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "cross_entropy  = nn.NLLLoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd476aa-3f20-491e-ad49-ebfdd55f0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    total_labels =[]\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_loader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_loader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        print (preds.logits)\n",
    "        print (labels)\n",
    "        print (preds.logits.shape)\n",
    "        print (labels.shape)\n",
    "        print (preds.logits.squeeze().shape)\n",
    "        print (labels.squeeze().shape)\n",
    "        loss = cross_entropy(preds.logits, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\"\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        # append the model predictions\n",
    "        total_preds+=list(preds)\n",
    "        total_labels+=labels.tolist()\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    f1 = f1_score(total_labels, total_preds, average='weighted')\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a08cc9-d641-4c5f-9f37-bc72a219da0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = train()\n",
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5d1ecb-6caa-411d-a243-51d1a8aa08c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd6beb4-8be3-48ac-ace8-a4eff6cd545d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3-8-19",
   "language": "python",
   "name": "python-3-8-19"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
